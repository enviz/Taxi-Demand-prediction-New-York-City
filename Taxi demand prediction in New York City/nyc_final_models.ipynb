{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all train and test data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Train_DF = pd.read_csv('Train_DF.csv')\n",
    "Test_DF = pd.read_csv('Test_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_TruePickups_flat.txt\",\"r\") as file:\n",
    "    train_TruePickups_flat=file.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_TruePickups_flat.txt\",\"r\") as file:\n",
    "    test_TruePickups_flat=file.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_TruePickups_flat = list(map(int, train_TruePickups_flat))\n",
    "test_TruePickups_flat = list(map(int, test_TruePickups_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_std = StandardScaler().fit_transform(Train_DF)\n",
    "test_std = StandardScaler().fit_transform(Test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#hyperparameter tuning\n",
    "clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\n",
    "values = [10**-4,10**-3,10**-2,10**-1,1,10,100,1000]\n",
    "hyper_parameter = {\"alpha\": values}\n",
    "best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "best_parameter.fit(train_std, train_TruePickups_flat)\n",
    "alpha = best_parameter.best_params_[\"alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\n",
    "clf.fit(train_std, train_TruePickups_flat)\n",
    "train_pred = clf.predict(train_std)\n",
    "lr_train_MAPE = mean_absolute_error(train_TruePickups_flat, train_pred)/ (sum(train_TruePickups_flat)/len(train_TruePickups_flat))\n",
    "lr_train_MSE = mean_squared_error(train_TruePickups_flat, train_pred)\n",
    "test_pred = clf.predict(test_std)\n",
    "lr_test_MAPE = mean_absolute_error(test_TruePickups_flat, test_pred)/ (sum(test_TruePickups_flat)/len(test_TruePickups_flat))\n",
    "lr_test_MSE = mean_squared_error(test_TruePickups_flat, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using Logistic regression \n",
      "train_MAPE: 0.13232113604281714\n",
      "train_MSE: 295.7725799623722\n",
      "test_MAPE: 0.13126781320135156\n",
      "test_MSE: 265.5542883192399\n"
     ]
    }
   ],
   "source": [
    "print(\" Using Logistic regression \")\n",
    "print(\"train_MAPE:\",lr_train_MAPE)\n",
    "print(\"train_MSE:\",lr_train_MSE)\n",
    "print(\"test_MAPE:\", lr_test_MAPE)\n",
    "print(\"test_MSE:\", lr_test_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "values = [10, 40, 80, 150, 600, 800]\n",
    "clf = RandomForestRegressor(n_jobs = -1)\n",
    "hyper_parameter = {\"n_estimators\": values}\n",
    "best_parameter = RandomizedSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "best_parameter.fit(Train_DF, train_TruePickups_flat)\n",
    "estimators = best_parameter.best_params_[\"n_estimators\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = estimators, n_jobs = -1)\n",
    "rf.fit(Train_DF, train_TruePickups_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = rf.predict(Train_DF)\n",
    "rf_train_MAPE = mean_absolute_error(train_TruePickups_flat, train_pred)/ (sum(train_TruePickups_flat)/len(train_TruePickups_flat))\n",
    "rf_train_MSE = mean_squared_error(train_TruePickups_flat, train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rf.predict(Test_DF)\n",
    "rf_test_MAPE = mean_absolute_error(test_TruePickups_flat, test_pred)/ (sum(test_TruePickups_flat)/len(test_TruePickups_flat))\n",
    "rf_test_MSE = mean_squared_error(test_TruePickups_flat, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using Random Forest:  \n",
      "train_MAPE: 0.0490885021833117\n",
      "train_MSE: 49.07482649474671\n",
      "test_MAPE: 0.11756638382556032\n",
      "test_MSE: 222.88876832252527\n"
     ]
    }
   ],
   "source": [
    "print(\" Using Random Forest:  \")\n",
    "print(\"train_MAPE:\",rf_train_MAPE)\n",
    "print(\"train_MSE:\",rf_train_MSE)\n",
    "print(\"test_MAPE:\", rf_test_MAPE)\n",
    "print(\"test_MSE:\", rf_test_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 1. The difference between train error and test error of random forest regressor is high, which clearly shows that random forest regressor is overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:12:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:13:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "hyper_parameters = {\"max_depth\":[1, 2, 3, 4], \"n_estimators\":[40, 80, 150, 600]}\n",
    "clf = xgb.XGBRegressor(n_jobs = -1)\n",
    "best_parameter = GridSearchCV(clf, hyper_parameters, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "best_parameter.fit(Train_DF, train_TruePickups_flat)\n",
    "estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "depth = best_parameter.best_params_[\"max_depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:13:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=4, min_child_weight=1, missing=None, n_estimators=80,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBRegressor(max_depth = depth, n_estimators = estimators)\n",
    "xgb_clf.fit(Train_DF, train_TruePickups_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_pred = xgb_clf.predict(Train_DF)\n",
    "xgb_train_MAPE = mean_absolute_error(train_TruePickups_flat, train_pred)/ (sum(train_TruePickups_flat)/len(train_TruePickups_flat))\n",
    "xgb_train_MSE = mean_squared_error(train_TruePickups_flat, train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = xgb_clf.predict(Test_DF)\n",
    "xgb_test_MAPE = mean_absolute_error(test_TruePickups_flat, test_pred)/ (sum(test_TruePickups_flat)/len(test_TruePickups_flat))\n",
    "xgb_test_MSE = mean_squared_error(test_TruePickups_flat, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using XGBoost model:  \n",
      "train_MAPE: 0.1276545079236112\n",
      "train_MSE: 260.3512640583455\n",
      "test_MAPE: 0.11741143576961738\n",
      "test_MSE: 220.7853657220282\n"
     ]
    }
   ],
   "source": [
    "print(\" Using XGBoost model:  \")\n",
    "print(\"train_MAPE:\",xgb_train_MAPE)\n",
    "print(\"train_MSE:\",xgb_train_MSE)\n",
    "print(\"test_MAPE:\", xgb_test_MAPE)\n",
    "print(\"test_MSE:\", xgb_test_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First we did some data cleaning on data and then removed outliers from data.\n",
    "\n",
    "2) After that we built a regression model using some features of frequencies and amplitudes from fast fourier transform of our time series data.\n",
    "\n",
    "3) And then we applied some regression algorithm such as linear regression, Random Forest Regressor, Xgboost Regressor and compared them using pretty table.\n",
    "\n",
    "4) In each model we performed hyperparameter tuning using GridSearch, RandomSearch and RandomSearch respectively.\n",
    "\n",
    "5) After observation we found that the Mean absolute percentage error for Xgboost is better as compared to our linear regression model and Random forest model but in the case of Random forest regressor the difference between train error and test error is very low as compared to other model.\n",
    "\n",
    "6)  The best model with lowest train and test error is XGBoost Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------------+\n",
      "|       Models      | MAPE train % | MAPE test % |\n",
      "+-------------------+--------------+-------------+\n",
      "| Linear Regression |     13.1     |     13.2    |\n",
      "|   Random Forest   |     4.9      |    11.75    |\n",
      "|      XGBOOST      |     12.7     |    11.74    |\n",
      "+-------------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x = PrettyTable([\"Models\", \"MAPE train %\", \"MAPE test %\"])\n",
    "x.add_row(['Linear Regression','13.1','13.2'])\n",
    "x.add_row(['Random Forest','4.9','11.75'])\n",
    "x.add_row(['XGBOOST','12.7','11.74'])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
